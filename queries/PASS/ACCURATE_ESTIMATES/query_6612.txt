
============================STATS========================================
Actual Count: 35
Estimated Count: 35
Abs diff: 0
============================QUERY========================================
select  
  ref_0.r_reason_sk as c0, 
  ref_0.r_reason_desc as c1
from 
  main.reason as ref_0
where ref_0.r_reason_desc is not NULL
limit 80
============================OUTPUT========================================
[c0: int, c1: string]
============================PLAN========================================
== Parsed Logical Plan ==
'GlobalLimit 80
+- 'LocalLimit 80
   +- 'Project ['ref_0.r_reason_sk AS c0#5902, 'ref_0.r_reason_desc AS c1#5903]
      +- 'Filter isnotnull('ref_0.r_reason_desc)
         +- 'SubqueryAlias ref_0
            +- 'UnresolvedRelation [main, reason]

== Analyzed Logical Plan ==
c0: int, c1: string
GlobalLimit 80
+- LocalLimit 80
   +- Project [r_reason_sk#834 AS c0#5902, r_reason_desc#836 AS c1#5903]
      +- Filter isnotnull(r_reason_desc#836)
         +- SubqueryAlias ref_0
            +- SubqueryAlias spark_catalog.main.reason
               +- Relation[r_reason_sk#834,r_reason_id#835,r_reason_desc#836] parquet

== Optimized Logical Plan ==
GlobalLimit 80
+- LocalLimit 80
   +- Project [r_reason_sk#834 AS c0#5902, r_reason_desc#836 AS c1#5903]
      +- Filter isnotnull(r_reason_desc#836)
         +- Relation[r_reason_sk#834,r_reason_id#835,r_reason_desc#836] parquet

== Physical Plan ==
CollectLimit 80
+- *(1) Project [r_reason_sk#834 AS c0#5902, r_reason_desc#836 AS c1#5903]
   +- *(1) Filter isnotnull(r_reason_desc#836)
      +- *(1) ColumnarToRow
         +- FileScan parquet main.reason[r_reason_sk#834,r_reason_desc#836] Batched: true, DataFilters: [isnotnull(r_reason_desc#836)], Format: Parquet, Location: InMemoryFileIndex[file:/home/ahmad/Documents/project/cardinality-estimator-test/tpcds-data-for-ma..., PartitionFilters: [], PushedFilters: [IsNotNull(r_reason_desc)], ReadSchema: struct<r_reason_sk:int,r_reason_desc:string>

