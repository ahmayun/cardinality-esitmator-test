
============================STATS========================================
Actual Count: 0
Estimated Count: 2
Abs diff: 2
============================QUERY========================================
select  
  ref_0.email as c0, 
  ref_0.c2 as c1, 
  ref_0.c2 as c2
from 
  main.users as ref_0
where ref_0.c1 is NULL
limit 135
============================OUTPUT========================================
[c0: string, c1: string ... 1 more field]
============================PLAN========================================
== Parsed Logical Plan ==
'GlobalLimit 135
+- 'LocalLimit 135
   +- 'Project ['ref_0.email AS c0#5485, 'ref_0.c2 AS c1#5486, 'ref_0.c2 AS c2#5487]
      +- 'Filter isnull('ref_0.c1)
         +- 'SubqueryAlias ref_0
            +- 'UnresolvedRelation [main, users]

== Analyzed Logical Plan ==
c0: string, c1: string, c2: string
GlobalLimit 135
+- LocalLimit 135
   +- Project [email#1142 AS c0#5485, c2#1141 AS c1#5486, c2#1141 AS c2#5487]
      +- Filter isnull(c1#1139)
         +- SubqueryAlias ref_0
            +- SubqueryAlias spark_catalog.main.users
               +- Relation[c0#1137,id#1138,c1#1139,name#1140,c2#1141,email#1142] parquet

== Optimized Logical Plan ==
GlobalLimit 135
+- LocalLimit 135
   +- Project [email#1142 AS c0#5485, c2#1141 AS c1#5486, c2#1141]
      +- Filter isnull(c1#1139)
         +- Relation[c0#1137,id#1138,c1#1139,name#1140,c2#1141,email#1142] parquet

== Physical Plan ==
CollectLimit 135
+- *(1) Project [email#1142 AS c0#5485, c2#1141 AS c1#5486, c2#1141]
   +- *(1) Filter isnull(c1#1139)
      +- *(1) ColumnarToRow
         +- FileScan parquet main.users[c1#1139,c2#1141,email#1142] Batched: true, DataFilters: [isnull(c1#1139)], Format: Parquet, Location: InMemoryFileIndex[file:/home/ahmad/Documents/project/cardinality-estimator-test/spark-warehouse/m..., PartitionFilters: [], PushedFilters: [IsNull(c1)], ReadSchema: struct<c1:string,c2:string,email:string>

