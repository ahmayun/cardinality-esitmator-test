
============================STATS========================================
Actual Count: 0
Estimated Count: 176
Abs diff: 176
============================QUERY========================================
select  
  ref_0.t_hour as c0
from 
  main.time_dim as ref_0
where ref_0.t_hour is NULL
limit 176
============================OUTPUT========================================
[c0: int]
============================PLAN========================================
== Parsed Logical Plan ==
'GlobalLimit 176
+- 'LocalLimit 176
   +- 'Project ['ref_0.t_hour AS c0#2358]
      +- 'Filter isnull('ref_0.t_hour)
         +- 'SubqueryAlias ref_0
            +- 'UnresolvedRelation [main, time_dim]

== Analyzed Logical Plan ==
c0: int
GlobalLimit 176
+- LocalLimit 176
   +- Project [t_hour#1105 AS c0#2358]
      +- Filter isnull(t_hour#1105)
         +- SubqueryAlias ref_0
            +- SubqueryAlias spark_catalog.main.time_dim
               +- Relation[t_time_sk#1102,t_time_id#1103,t_time#1104,t_hour#1105,t_minute#1106,t_second#1107,t_am_pm#1108,t_shift#1109,t_sub_shift#1110,t_meal_time#1111] parquet

== Optimized Logical Plan ==
GlobalLimit 176
+- LocalLimit 176
   +- Project [t_hour#1105 AS c0#2358]
      +- Filter isnull(t_hour#1105)
         +- Relation[t_time_sk#1102,t_time_id#1103,t_time#1104,t_hour#1105,t_minute#1106,t_second#1107,t_am_pm#1108,t_shift#1109,t_sub_shift#1110,t_meal_time#1111] parquet

== Physical Plan ==
CollectLimit 176
+- *(1) Project [t_hour#1105 AS c0#2358]
   +- *(1) Filter isnull(t_hour#1105)
      +- *(1) ColumnarToRow
         +- FileScan parquet main.time_dim[t_hour#1105] Batched: true, DataFilters: [isnull(t_hour#1105)], Format: Parquet, Location: InMemoryFileIndex[file:/home/ahmad/Documents/project/cardinality-estimator-test/tpcds-data-for-ma..., PartitionFilters: [], PushedFilters: [IsNull(t_hour)], ReadSchema: struct<t_hour:int>

